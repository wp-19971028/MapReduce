## 社交粉丝数据分析

### 综合案例：

需求：计算出来两两用户之间，有哪些共同好友？

思路：

```shell
#案例数据
A: B C D
B: C D
C: A B D
D: A C

#某一个用户，在哪些好友列表中存在
#先mapper任务
<k1,v1> => <k2,v2>

<k2,  v2>
 B    A
 C    A
 D    A
 C    B
 D    B
 A    C
 B    C
 D    C
 A    D
 C    D
#reduce任务
 B:{A,C}
 C:{A,B,D}
 D:{A,B,C}
 A:{C,D}
 
 #写第二个MR
 #mapper任务
 <k2, V2>
 A-B  C
 A-B  D
 A-C  B
 A-C  D
 A-D  C
 B-C  D
 B-D  C
 C-D  A
 #reduce任务，先分组在聚合
 A-B:{C,D}
 A-C:{B,D}
 A-D:{C}
 B-C:{D}
 B-D:{C}
 C-D:{A}
```

#### 代码实现

- Friend1MapperTask 实现

```java
package demo10;



import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class Friend1MapperTask extends Mapper<LongWritable, Text,Text,Text> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        // 读取每一行数据
        String line = value.toString();
        // 判断每一行是否存在
        if (StringUtils.isNotEmpty(line)){
            // 对数据进行切割
            String[] splits = line.split(":");
            String[] friends = splits[1].split(",");
            // 遍历好友
            for (String friend:friends){
                // 写出去
                context.write(new Text(friend),new Text(splits[0]));
            }
        }
    }
}
```

```java
package demo10;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class Friend1ReducerTask extends Reducer<Text,Text,Text,Text> {
    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        String v3 = "";
        // 1. 遍历好友信息列表
        for (Text friend:values) {
            v3 += friend + "-";
        }
        // 2. 写出去
        context.write(key,new Text(v3));
    }
}
```

```java
package demo10;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class Friend1JobMain {
    public static void main(String[] args) throws Exception {
        // 1. 创建job对象
        Job job = Job.getInstance(new Configuration(), "Friend1JobMR");
        // 2. 组装八大步
        //2.1输入
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.addInputPath(job,new Path("E:\\MapReduce\\MapReduce\\共同好友\\input\\friends.txt"));
        //2.2设置mapper
        job.setMapperClass(Friend1MapperTask.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        //2.3 shuffle 分区 排序 规约 分组
        //2.7 设置reduce
        job.setReducerClass(Friend1ReducerTask.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        //2.8 输出
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job,new Path("E:\\MapReduce\\MapReduce\\共同好友\\output"));
        //3 提交任务
        boolean flag = job.waitForCompletion(true);
        //4 退出执行
        System.exit(flag?0:1);
    }
}
```





```java
package demo11;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;
import java.util.Arrays;

public class Friend2MapperTask extends Mapper<LongWritable, Text,Text,Text> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        // 判空
        if(StringUtils.isNotEmpty(line)){
            // 切割
            String[] splits = line.split("\t");
            String[] friends = splits[1].split("-");
            // 从小到大排序
            Arrays.sort(friends);
            //遍历 A  B-C-F-G-H-I-K-O
            for(int i=0;i<friends.length-1;i++){
                for(int j=i+1;j<friends.length;j++){
                    String k2 = friends[i] + "-" + friends[j];
                    //写出去
                    context.write(new Text(k2),new Text(splits[0]));
                }
            }
        }
    }
}
```

```java
package demo11;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class Friend2ReducerTask extends Reducer<Text,Text,Text,Text> {
    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        String v3="";
        //1.遍历
        for(Text value:values){
            v3 += value+",";
        }
        //2.写出去
        context.write(key, new Text(v3));
    }
}
```

```java
package demo11;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class Friend2JobMain {
    public static void main(String[] args) throws Exception {
        //1.创建job对象
        Job job = Job.getInstance(new Configuration(), "Friend2JobMR");
        //2.组装八大步
        //2.1输入
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.addInputPath(job,new Path("E:\\MapReduce\\MapReduce\\共同好友\\output"));
        //2.2设置mapper
        job.setMapperClass(Friend2MapperTask.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        //2.3 shuffle 分区 排序 规约 分组
        //2.7 设置reduce
        job.setReducerClass(Friend2ReducerTask.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        //2.8 输出
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job,new Path("E:\\MapReduce\\MapReduce\\共同好友\\output2"));
        //3 提交任务
        boolean flag = job.waitForCompletion(true);
        //4 退出执行
        System.exit(flag?0:1);
    }
}
```

